Majority Pipeline Notes:
======================

Pipeline:
===========
LSH -> Distribution Evaluation (both device and error)  (done)
    -> Error rate evaluation (done)
    -> within bucket similarity evaluation (done)


LSH -> cluster -> Evaluation


LSH -> hash evaluation
    





Problems
=========
1. Now we're only using one set of buckets. Although we can tune the hash function to make error rate very low. That only means the false positive is low, how about false negative?  We still need do ground-truth validation right? Random draw devices from different buckets, and do ground-truth evaluation?

2. 





Results Log
==================
Version 1.x
===========
0. evaluation_distribution local version3.0:
   input: LSH version 3.0
   output: evaluation_distribution version 3.0
   evaluation with three conditions and location is half-right condition

1. evaluation_pipeline version 1.0
   evaluation with only timestamp for 12m buckets
   => Good: cluster with small device (i.e. 2~4) has low error rate
   => Bad: lots of high similarity(e.g. 1.0) are still wrong

   observation:
   10000157_12m	7257,0,0,2014-04-08 15:36:21.777|2014-04-08 15:47:52.031|2014-04-08 15:47:16.621|2014-04-08 15:36:07.624|2014-04-08 15:35:00.737|2014-04-08 15:36:22.258,29,Lakeside,363,11850,0,19,0_0,0,0,0,1,NA,NA,419024268,1,NULL,cdn.net-mine.com,85,1,1,false,41436,g0,null,false,Cox Communications,NULL,xjyOakwlVuTyBr

	10000157_12m	7257,0,0,2014-04-07 21:10:29.147|2014-04-07 21:14:47.219|2014-04-07 21:19:17.786|2014-04-07 21:20:52.608|2014-04-07 21:23:09.927|2014-04-07 21:10:18.711|2014-04-07 21:04:38.215|2014-04-07 21:04:36.088|2014-04-07 21:23:08.964|2014-04-07 21:19:16.958,29,Encinitas,363,11850,0,19,0_0,0,0,0,1,NA,NA,2924324601,1,NULL,cdn.net-mine.com,85,1,1,false,41436,g0,null,false,Cox Communications,NULL,y3RKid2tBZfi9w



2. evaluation_pipeline version 1.1
   -> also evaluation with only timestamp for 12m bucket
   -> but timestamp failure condition is relaxed to 20% overlap
   -> also reformat the output format, now have similarity distribution within each user_type

3. evaluation pipeline version 1.2
  -> same as version1.1, but further relax the timestamp to 50%
  -> in order to solve big cluster timeout problem, now random select 30 devices from a cluster

4. evaluation pipeline version 1.3
  -> only evaluate hid, tolerance factor 0.2

5. evaluation pipeline version 1.4
  -> only location, tolearance factor 0.025 ~ 25-30 miles

6. evaluation pipeline version 1.5
  -> only location, tolerance factor 0.03




Version 2.x
==============
1. version 2.0: 
   -> tuned lsh parameters
   -> evaluation with only version1.5's evaluation method (only location with tolerance factor 0.03)
   -> note: for city comparison, also leverage the random 30 technique to avoid big city_list

2. 



Version 3.x
==========
1. temporarily for mixed set
2. 


Visualization:  should have a rough image of these plots
==============
1. number of users that has one/two/device....  
	(done) => should be a power law
2. percentage of users that has one/two/devices.. are wrong  
    (done) => should be an increasing curve
3. Similarity distribution for both correct and error users
   for each type of user (i.e. user with one/two/devices)
   => better than average similarity of each user type


